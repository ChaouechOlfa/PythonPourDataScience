{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6a037a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipuler les données quantitatives avec numpy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3083b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple\n",
    "ar=np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e1d12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a66754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a329676d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d752b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension= 1 taille= 4 forme= (4,)\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension=\",ar.ndim,\"taille=\",ar.size,\"forme=\",ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d25dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "332dd004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar[-1]= 4 et ar[-2]= 3\n"
     ]
    }
   ],
   "source": [
    "#Indexer à partir de la fin du tableau\n",
    "print(\"ar[-1]=\",ar[-1],\"et ar[-2]=\",ar[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254bffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tableau multidimensionnel\n",
    "ar1=np.array([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c23ef6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "398fb101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour accéder à l'élément quatre\n",
    "ar1[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f9c602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifiant la valeur d'un élément donné\n",
    "ar1[0,0]=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9feb4772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa8f64f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  6,  9, 12, 15, 18])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Découpage\n",
    "#Exemple\n",
    "ar2=np.arange(0,20,3)\n",
    "ar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df2477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#le tableau \"ar3\" contenant les cinq premiers éléments de \"ar2\"\n",
    "ar3=ar2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc2c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  6,  9, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f72c1ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Découpage de ar2 jusqu'à l'élément 3 et avec un pas de 2\n",
    "ar3[:4:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86f32a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 6, 3, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nous allons jusqu'à l'indice trois puis nous inversons notre tableau\n",
    "ar3[3::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f10f2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remodelage\n",
    "ar4=ar1.reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e56781c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52a5dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aborder les techniques d'encodage\n",
    "df=pd.DataFrame({'Salarié':['Eric','Elise','Hapsa','Xin'],'Genre':['Homme','Femme','Femme','Homme'],'groupe':['Technicien','Ingénieur','PhD','Technicien'],'Date_embauche':[2016,1988,2012,2001],'Salaire':[30000,100000,60000,60000]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22ee66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une Copie de notre DataFrame\n",
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36df12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodage numérique avec pandas\n",
    "pré_traitement={\"groupe\":{\"PhD\":1,\"Ingénieur\":2,\"Technicien\":3}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ea40a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salarié</th>\n",
       "      <th>Genre</th>\n",
       "      <th>groupe</th>\n",
       "      <th>Date_embauche</th>\n",
       "      <th>Salaire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eric</td>\n",
       "      <td>Homme</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elise</td>\n",
       "      <td>Femme</td>\n",
       "      <td>2</td>\n",
       "      <td>1988</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hapsa</td>\n",
       "      <td>Femme</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xin</td>\n",
       "      <td>Homme</td>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Salarié  Genre  groupe  Date_embauche  Salaire\n",
       "0    Eric  Homme       3           2016    30000\n",
       "1   Elise  Femme       2           1988   100000\n",
       "2   Hapsa  Femme       1           2012    60000\n",
       "3     Xin  Homme       3           2001    60000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.replace(pré_traitement,inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb1d8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodage numérique avec scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['groupe_encodé']=LabelEncoder().fit_transform(df['groupe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d71767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour modifier l'ordre qui est par défaut l'ordre alphabétique, nous allons utiliser la fct Map de pandas\n",
    "df['groupe_encodé']=df['groupe'].map({'PhD':1,'Ingénieur':2,'Technicien':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93b1ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>groupe</th>\n",
       "      <th>groupe_encodé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technicien</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ingénieur</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PhD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Technicien</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       groupe  groupe_encodé\n",
       "0  Technicien              3\n",
       "1   Ingénieur              2\n",
       "2         PhD              1\n",
       "3  Technicien              3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['groupe','groupe_encodé']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34164a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre_Femme</th>\n",
       "      <th>Genre_Homme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre_Femme  Genre_Homme\n",
       "0            0            1\n",
       "1            1            0\n",
       "2            1            0\n",
       "3            0            1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encodage one-hot\n",
    "pd.get_dummies(df['Genre'],prefix='Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2859d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L'avantage de l'encodage One-hot est de ne pas pondérer une valeur de manière incorrecte. Néanmoins, l'inconvénient est d'ajouter plus de colonnes à l'ensemble de données\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"L'avantage de l'encodage One-hot est de ne pas pondérer une valeur de manière incorrecte. Néanmoins, l'inconvénient est d'ajouter plus de colonnes à l'ensemble de données\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a76f2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipuler les données textuelles avec pandas\n",
    "données=['valérie','Omar','MAHA','HAPSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8af8de75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Valérie', 'Omar', 'Maha', 'Hapsa']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardiser les données en utilisant la fonction capitalize\n",
    "[s.capitalize() for s in données]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c37359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mais le problème avec la fonction capitalize de python, c'est que dès que notre liste contient par exemple un élément vide:\n",
    "None, alors une erreur est affichée indiquant que le type None n a pas d'attribut capitalize.\n",
    "Pour faire face à cela, nous allons alors utiliser la librairie pandas\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd3e4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "données=['valérie','Omar','MAHA','haspsa',None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f33b84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    valérie\n",
       "1       Omar\n",
       "2       MAHA\n",
       "3     haspsa\n",
       "4       None\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "prénom=pd.Series(données)#Nous créons une série dans laquelle nous allons stocker nos données\n",
    "prénom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f9191e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Valérie\n",
       "1       Omar\n",
       "2       Maha\n",
       "3     Haspsa\n",
       "4       None\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prénom.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e2d028b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    valérie\n",
       "1       omar\n",
       "2       maha\n",
       "3     haspsa\n",
       "4       None\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ecrire tout en miniscules avec la fonction lower\n",
    "prénom.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3dd4143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    VALÉRIE\n",
       "1       OMAR\n",
       "2       MAHA\n",
       "3     HASPSA\n",
       "4       None\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ecrire tout en majuscule avec la fonction upper\n",
    "prénom.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a0bd79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7.0\n",
       "1    4.0\n",
       "2    4.0\n",
       "3    6.0\n",
       "4    NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Il est possible de calculer la longueur de la chaine en utilisant la fonction length\n",
    "prénom.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d3e94a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    valérie\n",
       "1       Omar\n",
       "2       MAHA\n",
       "3     haspsa\n",
       "4       None\n",
       "5     Oliver\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ou encore, rajouter un nouveau prénom à notre série\n",
    "prénom[5]=\"Oliver\"\n",
    "prénom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86b06131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4     None\n",
       "5    False\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La fonction startswith qui indique la valeur true quand le prénom commence par o et false sinon\n",
    "prénom.str.startswith('o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d3d6f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omar\n",
      "Oliver\n"
     ]
    }
   ],
   "source": [
    "\"\"\"La fonction startswith() est une fonction très intéressante puisqu'elle nous permet de filtrer par exemple, uniquement \n",
    "un certain nombre d'élément. D'ailleurs, regardons ici un affichage en utilisant la boucle for\"\"\"\n",
    "for p in prénom:\n",
    "    if p!= None and p.startswith('O')==True:print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10cd6056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Omar So\n",
       "1     Maha To\n",
       "2    Hapsa Di\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Une autre fonction très utile avec pandas est le Split(): ça permet de composer par exemple un nom et prénom qui seraient \n",
    "#attachés en mettant le prénom d'un côté et le nom d'un autre côté\n",
    "#Exp:\n",
    "prénom=pd.Series(['Omar So','Maha To','Hapsa Di'])\n",
    "prénom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8546eac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [Omar, So]\n",
       "1     [Maha, To]\n",
       "2    [Hapsa, Di]\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prénom.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4139002f",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "bad character range a-Z at position 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Par ailleurs, il est possible d'extraire uniquement une partie du texte qui nous intéresserait, par exemple ici dans notre cas,\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#nous allons nous intéresser à l'extraction uniquement du prénom \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mprénom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m([A-Za-Z]+)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:129\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:2607\u001b[0m, in \u001b[0;36mStringMethods.extract\u001b[1;34m(self, pat, flags, expand)\u001b[0m\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expand, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m   2605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand must be True or False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2607\u001b[0m regex \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regex\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern contains no capture groups\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\re.py:251\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(pattern, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 303\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    787\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 788\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    790\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\sre_parse.py:955\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    952\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[0;32m    959\u001b[0m     state \u001b[38;5;241m=\u001b[39m State()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    442\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\sre_parse.py:841\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(err\u001b[38;5;241m.\u001b[39mmsg, \u001b[38;5;28mlen\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    839\u001b[0m sub_verbose \u001b[38;5;241m=\u001b[39m ((verbose \u001b[38;5;129;01mor\u001b[39;00m (add_flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    840\u001b[0m                \u001b[38;5;129;01mnot\u001b[39;00m (del_flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[1;32m--> 841\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing ), unterminated subpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    844\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    442\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\sre_parse.py:599\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hi \u001b[38;5;241m<\u001b[39m lo:\n\u001b[0;32m    598\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad character range \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (this, that)\n\u001b[1;32m--> 599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(msg, \u001b[38;5;28mlen\u001b[39m(this) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(that))\n\u001b[0;32m    600\u001b[0m     setappend((RANGE, (lo, hi)))\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31merror\u001b[0m: bad character range a-Z at position 5"
     ]
    }
   ],
   "source": [
    "#Par ailleurs, il est possible d'extraire uniquement une partie du texte qui nous intéresserait, par exemple ici dans notre cas,\n",
    "#nous allons nous intéresser à l'extraction uniquement du prénom \n",
    "prénom.str.extract('([A-Za-Z]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90f5a642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hapsa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0   Omar\n",
       "1   Maha\n",
       "2  Hapsa"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prénom.str.extract('([a-zA-Z]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9366f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipuler des données textuelles avec NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b090a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Description=open(\"exemple_texte.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14b027a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='exemple_texte.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8aa2d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De nos jours, l'explosion du volume et de la variÃ©tÃ© des donnÃ©es ne laisse plus planer de doute : le rÃ´le du data scientist sera central dans les annÃ©es Ã  venir. Si vous Ãªtes data scientist ou que vous souhaitez travailler avec Python, ce cours est pour vous. Omar Souissi, professeur associÃ© en technologie de l'information et techniques d'optimisation, vous aide Ã  acquÃ©rir les bases indispensables pour faire de la data science avec Python. Vous dÃ©couvrirez comment utiliser deux bibliothÃ¨ques indispensables, Ã  savoir NumPy et Pandas. Vous verrez Ã©galement quelques Ã©tudes de cas qui vous permettront d'assimiler facilement les diffÃ©rentes notions de Python pour l'analyse de donnÃ©es\n"
     ]
    }
   ],
   "source": [
    "Contenu = Description.read()\n",
    "print (Contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d2175d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Découpage en mots\n",
    "token=Contenu.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d3b83019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['De',\n",
       " 'nos',\n",
       " 'jours,',\n",
       " \"l'explosion\",\n",
       " 'du',\n",
       " 'volume',\n",
       " 'et',\n",
       " 'de',\n",
       " 'la',\n",
       " 'variÃ©tÃ©',\n",
       " 'des',\n",
       " 'donnÃ©es',\n",
       " 'ne',\n",
       " 'laisse',\n",
       " 'plus',\n",
       " 'planer',\n",
       " 'de',\n",
       " 'doute',\n",
       " ':',\n",
       " 'le',\n",
       " 'rÃ´le',\n",
       " 'du',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'sera',\n",
       " 'central',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'annÃ©es',\n",
       " 'Ã\\xa0',\n",
       " 'venir.',\n",
       " 'Si',\n",
       " 'vous',\n",
       " 'Ãªtes',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'ou',\n",
       " 'que',\n",
       " 'vous',\n",
       " 'souhaitez',\n",
       " 'travailler',\n",
       " 'avec',\n",
       " 'Python,',\n",
       " 'ce',\n",
       " 'cours',\n",
       " 'est',\n",
       " 'pour',\n",
       " 'vous.',\n",
       " 'Omar',\n",
       " 'Souissi,',\n",
       " 'professeur',\n",
       " 'associÃ©',\n",
       " 'en',\n",
       " 'technologie',\n",
       " 'de',\n",
       " \"l'information\",\n",
       " 'et',\n",
       " 'techniques',\n",
       " \"d'optimisation,\",\n",
       " 'vous',\n",
       " 'aide',\n",
       " 'Ã\\xa0',\n",
       " 'acquÃ©rir',\n",
       " 'les',\n",
       " 'bases',\n",
       " 'indispensables',\n",
       " 'pour',\n",
       " 'faire',\n",
       " 'de',\n",
       " 'la',\n",
       " 'data',\n",
       " 'science',\n",
       " 'avec',\n",
       " 'Python.',\n",
       " 'Vous',\n",
       " 'dÃ©couvrirez',\n",
       " 'comment',\n",
       " 'utiliser',\n",
       " 'deux',\n",
       " 'bibliothÃ¨ques',\n",
       " 'indispensables,',\n",
       " 'Ã\\xa0',\n",
       " 'savoir',\n",
       " 'NumPy',\n",
       " 'et',\n",
       " 'Pandas.',\n",
       " 'Vous',\n",
       " 'verrez',\n",
       " 'Ã©galement',\n",
       " 'quelques',\n",
       " 'Ã©tudes',\n",
       " 'de',\n",
       " 'cas',\n",
       " 'qui',\n",
       " 'vous',\n",
       " 'permettront',\n",
       " \"d'assimiler\",\n",
       " 'facilement',\n",
       " 'les',\n",
       " 'diffÃ©rentes',\n",
       " 'notions',\n",
       " 'de',\n",
       " 'Python',\n",
       " 'pour',\n",
       " \"l'analyse\",\n",
       " 'de',\n",
       " 'donnÃ©es']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40acc4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chaou/nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mContenu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\chaou/nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\chaou\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.word_tokenize(Contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2785f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chaou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['De',\n",
       " 'nos',\n",
       " 'jours',\n",
       " ',',\n",
       " \"l'explosion\",\n",
       " 'du',\n",
       " 'volume',\n",
       " 'et',\n",
       " 'de',\n",
       " 'la',\n",
       " 'variÃ©tÃ©',\n",
       " 'des',\n",
       " 'donnÃ©es',\n",
       " 'ne',\n",
       " 'laisse',\n",
       " 'plus',\n",
       " 'planer',\n",
       " 'de',\n",
       " 'doute',\n",
       " ':',\n",
       " 'le',\n",
       " 'rÃ´le',\n",
       " 'du',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'sera',\n",
       " 'central',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'annÃ©es',\n",
       " 'Ã',\n",
       " 'venir',\n",
       " '.',\n",
       " 'Si',\n",
       " 'vous',\n",
       " 'Ãªtes',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'ou',\n",
       " 'que',\n",
       " 'vous',\n",
       " 'souhaitez',\n",
       " 'travailler',\n",
       " 'avec',\n",
       " 'Python',\n",
       " ',',\n",
       " 'ce',\n",
       " 'cours',\n",
       " 'est',\n",
       " 'pour',\n",
       " 'vous',\n",
       " '.',\n",
       " 'Omar',\n",
       " 'Souissi',\n",
       " ',',\n",
       " 'professeur',\n",
       " 'associÃ©',\n",
       " 'en',\n",
       " 'technologie',\n",
       " 'de',\n",
       " \"l'information\",\n",
       " 'et',\n",
       " 'techniques',\n",
       " \"d'optimisation\",\n",
       " ',',\n",
       " 'vous',\n",
       " 'aide',\n",
       " 'Ã',\n",
       " 'acquÃ©rir',\n",
       " 'les',\n",
       " 'bases',\n",
       " 'indispensables',\n",
       " 'pour',\n",
       " 'faire',\n",
       " 'de',\n",
       " 'la',\n",
       " 'data',\n",
       " 'science',\n",
       " 'avec',\n",
       " 'Python',\n",
       " '.',\n",
       " 'Vous',\n",
       " 'dÃ©couvrirez',\n",
       " 'comment',\n",
       " 'utiliser',\n",
       " 'deux',\n",
       " 'bibliothÃ¨ques',\n",
       " 'indispensables',\n",
       " ',',\n",
       " 'Ã',\n",
       " 'savoir',\n",
       " 'NumPy',\n",
       " 'et',\n",
       " 'Pandas',\n",
       " '.',\n",
       " 'Vous',\n",
       " 'verrez',\n",
       " 'Ã©galement',\n",
       " 'quelques',\n",
       " 'Ã©tudes',\n",
       " 'de',\n",
       " 'cas',\n",
       " 'qui',\n",
       " 'vous',\n",
       " 'permettront',\n",
       " \"d'assimiler\",\n",
       " 'facilement',\n",
       " 'les',\n",
       " 'diffÃ©rentes',\n",
       " 'notions',\n",
       " 'de',\n",
       " 'Python',\n",
       " 'pour',\n",
       " \"l'analyse\",\n",
       " 'de',\n",
       " 'donnÃ©es']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.word_tokenize(Contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cfcaf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.RegexpTokenizer('r\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1acf9b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rs',\n",
       " 'riÃ',\n",
       " 'rÃ',\n",
       " 'ra',\n",
       " 'ral',\n",
       " 'ravailler',\n",
       " 'rs',\n",
       " 'rofesseur',\n",
       " 'rmation',\n",
       " 'rir',\n",
       " 're',\n",
       " 'rirez',\n",
       " 'rrez',\n",
       " 'rmettront',\n",
       " 'rentes']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sac_de_mots=tokenizer.tokenize(Contenu)\n",
    "Sac_de_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2424caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('french')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da42bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chaou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2d9c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtre=[]\n",
    "for w in Sac_de_mots[1:]:\n",
    "    if w not in stopwords:\n",
    "        Filtre.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6691a5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['riÃ',\n",
       " 'rÃ',\n",
       " 'ra',\n",
       " 'ral',\n",
       " 'ravailler',\n",
       " 'rs',\n",
       " 'rofesseur',\n",
       " 'rmation',\n",
       " 'rir',\n",
       " 're',\n",
       " 'rirez',\n",
       " 'rrez',\n",
       " 'rmettront',\n",
       " 'rentes']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85899df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filtre.count('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
